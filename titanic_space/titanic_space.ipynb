{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-05T14:42:13.221133300Z",
     "start_time": "2024-02-05T14:42:13.203132800Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "import category_encoders as ce\n",
    "# import missingno as msno\n",
    "import category_encoders as ce\n",
    "from sklearn.metrics import (roc_auc_score, recall_score, f1_score, precision_score,\n",
    "                             accuracy_score)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from catboost import Pool, cv\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers as L         # Уже готовые слои для моделей\n",
    "from tensorflow.keras.models import Sequential   # Специальный класс для склеивания слоёв\n",
    "from tensorflow.keras.models import Model        # Альтернативный класс для склейки слоёв\n",
    "import tensorflow.keras.optimizers as opt        # Разные оптимизационные алгоритмы :3\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1 About dataset\n",
    "\n",
    "This dataset is taken form kaggle (https://www.kaggle.com/competitions/spaceship-titanic)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "train.csv - Personal records for about two-thirds (~8700) of the passengers, to be used as training data.\n",
    "  * PassengerId - A unique Id for each passenger. Each Id takes the form gggg_pp where gggg indicates a group the passenger is travelling with and pp is their number within the group. People in a group are often family members, but not always.\n",
    "  * HomePlanet - The planet the passenger departed from, typically their planet of permanent residence.\n",
    "  * CryoSleep - Indicates whether the passenger elected to be put into suspended animation for the duration of the voyage. Passengers in cryosleep are confined to their cabins.\n",
    "  * Cabin - The cabin number where the passenger is staying. Takes the form deck/num/side, where side can be either P for Port or S for Starboard.\n",
    "  * Destination - The planet the passenger will be debarking to.\n",
    "  * Age - The age of the passenger.\n",
    "  * VIP - Whether the passenger has paid for special VIP service during the voyage.\n",
    "RoomService, FoodCourt, ShoppingMall, Spa, VRDeck - Amount the passenger has billed at each of the Spaceship Titanic's many luxury amenities.\n",
    "  * Name - The first and last names of the passenger.\n",
    "  * Transported - Whether the passenger was transported to another dimension. This is the target, the column you are trying to predict."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "\n",
    "# Load train and test data\n",
    "data_raw = pd.read_csv('train.csv')\n",
    "data_raw_test = pd.read_csv('test.csv')\n",
    "\n",
    "# Display train data\n",
    "data_raw"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-02-05T14:42:14.417003900Z",
     "start_time": "2024-02-05T14:42:14.346009200Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Check for missing values in each column\n",
    "{key: data_raw[key].isna().sum() for key in data_raw.columns}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-05T14:42:15.068607100Z",
     "start_time": "2024-02-05T14:42:15.029612100Z"
    }
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# Make copies of the original data\n",
    "data_pre = data_raw.copy()\n",
    "data_pre_test = data_raw_test.copy()\n",
    "\n",
    "# Fill missing values in 'Age' column with median\n",
    "median_age = data_pre['Age'].median()\n",
    "median_age_test = data_pre_test['Age'].median()\n",
    "\n",
    "data_pre['Age'].fillna(median_age, inplace=True)\n",
    "data_pre_test['Age'].fillna(median_age_test, inplace=True)\n",
    "\n",
    "def grouping_rule(data_pre, data_column):\n",
    "    \"\"\"\n",
    "    Function to fill missing values in categorical columns based on grouping rules.\n",
    "    \"\"\"\n",
    "    groups_with_nan = data_pre[(data_pre[data_column].isna() == True) & (data_pre['grid'] > 1)]['grp']\n",
    "    groups_with_nan = list(groups_with_nan)\n",
    "    for i in range(len(groups_with_nan)):\n",
    "        planet_group_qty = set(data_pre[data_pre['grp'] == groups_with_nan[i]][data_column])\n",
    "        planet_group_qty = list({x for x in planet_group_qty if x == x})\n",
    "        if len(planet_group_qty) == 0:\n",
    "            planet_fill_in = data_pre[:][data_column].mode()[0]\n",
    "            data_pre.loc[data_pre['grp'] == groups_with_nan[i], data_column] = \\\n",
    "                data_pre[data_pre['grp'] == groups_with_nan[i]][data_column].replace(np.nan, planet_fill_in)\n",
    "        else:\n",
    "            planet_fill_in = data_pre[data_pre['grp'] == groups_with_nan[i]][data_column].mode()[0]\n",
    "            data_pre.loc[data_pre['grp'] == groups_with_nan[i], data_column] = \\\n",
    "                data_pre[data_pre['grp'] == groups_with_nan[i]][data_column].replace(np.nan, planet_fill_in)\n",
    "        print(data_pre.loc[data_pre['grp'] == groups_with_nan[i], data_column])\n",
    "\n",
    "    planet_fill_in_for_df = data_pre[:][data_column].mode()[0]\n",
    "    data_pre[data_column] = data_pre[data_column].replace(np.nan, planet_fill_in_for_df)\n",
    "\n",
    "def ohe_data(data_pre, column):\n",
    "    \"\"\"\n",
    "    Function to perform one-hot encoding for categorical columns.\n",
    "    \"\"\"\n",
    "    for i in column:\n",
    "        one_hot = pd.get_dummies(data_pre[i])\n",
    "        data_pre = data_pre.join(one_hot)\n",
    "        data_pre = data_pre.drop(i, axis=1)\n",
    "    return data_pre\n",
    "\n",
    "# Split 'PassengerId' column into 'grp' and 'grid'\n",
    "data_pre[['grp', 'grid']] = data_pre['PassengerId'].str.split('_', expand=True)\n",
    "data_pre['grp'] = data_pre['grp'].astype(int)\n",
    "data_pre['grid'] = data_pre['grid'].astype(int)\n",
    "\n",
    "# Apply grouping rule for missing values in categorical columns\n",
    "grouping_rule(data_pre, data_column='HomePlanet')\n",
    "grouping_rule(data_pre, data_column='Destination')\n",
    "grouping_rule(data_pre, data_column='Cabin')\n",
    "grouping_rule(data_pre, data_column='VIP')\n",
    "grouping_rule(data_pre, data_column='CryoSleep')\n",
    "\n",
    "# Extract 'deck', 'num', and 'side' from 'Cabin' column and drop unnecessary columns\n",
    "data_pre[['deck', 'num', 'side']] = data_pre['Cabin'].str.split('/', expand=True)\n",
    "data_pre.drop(['PassengerId', 'Name', 'Cabin'], axis=1, inplace=True)\n",
    "\n",
    "# Check for missing values after data preprocessing\n",
    "{key: data_pre[key].isna().sum() for key in data_pre.columns}\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-02-05T14:42:16.706899200Z",
     "start_time": "2024-02-05T14:42:15.694904600Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Repeat the same preprocessing steps for test data\n",
    "data_pre_test[['grp', 'grid']] = data_pre_test['PassengerId'].str.split('_', expand=True)\n",
    "data_pre_test['grp'] = data_pre_test['grp'].astype(int)\n",
    "data_pre_test['grid'] = data_pre_test['grid'].astype(int)\n",
    "\n",
    "grouping_rule(data_pre=data_pre_test, data_column='HomePlanet')\n",
    "grouping_rule(data_pre=data_pre_test, data_column='Destination')\n",
    "grouping_rule(data_pre=data_pre_test, data_column='Cabin')\n",
    "grouping_rule(data_pre=data_pre_test, data_column='VIP')\n",
    "grouping_rule(data_pre=data_pre_test, data_column='CryoSleep')\n",
    "\n",
    "data_pre_test[['deck', 'num', 'side']] = data_pre_test['Cabin'].str.split('/', expand=True)\n",
    "data_pre_test.drop(['PassengerId', 'Name', 'Cabin'], axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-05T14:42:17.184899300Z",
     "start_time": "2024-02-05T14:42:16.705898600Z"
    }
   },
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2 Model Building\n",
    "Optimization for various metrics\n",
    "Different types of cross-validation\n",
    "Different preprocessing techniques for categorical features\n",
    "Various hyperparameter tuning methods\n",
    "Feature selection\n",
    "KNN, linear models, linear models with regularization, ensembles (random forest, 3 types of boosting algorithms from 3 different companies)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "# Splitting the data into features (X) and target variable (y)\n",
    "y = data_pre['Transported']\n",
    "X = data_pre.drop(['Transported'], axis=1)\n",
    "\n",
    "# Splitting the data into training and holdout sets\n",
    "X_train, X_holdout, y_train, y_holdout = train_test_split(X, y, test_size=0.3, random_state=17)\n",
    "\n",
    "# Defining categorical and numeric features\n",
    "categorical_features = ['deck', 'side', 'HomePlanet', 'Destination']\n",
    "numeric_features = [i for i in X_train.columns if i not in categorical_features]\n",
    "\n",
    "# Preprocessing pipelines for numeric and categorical features\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('ohe', ce.OneHotEncoder(use_cat_names=True))\n",
    "])\n",
    "\n",
    "# ColumnTransformer to apply preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Preprocessing the training and holdout data\n",
    "X_train_prep = preprocessor.fit_transform(X_train)\n",
    "X_holdout_prep = preprocessor.transform(X_holdout)\n",
    "\n",
    "# Defining models and hyperparameters for RandomizedSearchCV\n",
    "\n",
    "# Logistic Regression\n",
    "param_dict_logistic = {'clf__C': np.linspace(0.01, 10, 1000)}\n",
    "\n",
    "# Random Forest\n",
    "param_dict_rndforest = {'clf__max_depth': np.arange(1, 10),\n",
    "                        'clf__min_samples_leaf': np.arange(1, 10),\n",
    "                        'clf__n_estimators': [100, 200, 300]\n",
    "                        }\n",
    "\n",
    "# KNN\n",
    "param_dist_knn = {'clf__n_neighbors': np.arange(1, 20),\n",
    "                  'clf__p': np.arange(1, 5)\n",
    "                  }\n",
    "\n",
    "# CatBoost\n",
    "param_dict_catboost = {\n",
    "    'clf__n_estimators': [100, 200, 300],  # Define n_estimators for the CatBoostClassifier\n",
    "    'clf__max_depth': np.arange(1, 10),\n",
    "    'clf__learning_rate': np.linspace(0.01, 0.3, 10),\n",
    "    'clf__l2_leaf_reg': np.linspace(0.01, 0.5, 10),\n",
    "    'clf__min_data_in_leaf': np.arange(1, 10)\n",
    "}\n",
    "\n",
    "# XGBoost\n",
    "param_dict_xgb = {\n",
    "    'clf__n_estimators': [100, 200, 300],\n",
    "    'clf__max_depth': np.arange(1, 10),\n",
    "    'clf__learning_rate': np.linspace(0.01, 0.3, 10),\n",
    "    'clf__reg_lambda': np.linspace(0.01, 0.5, 10),\n",
    "    'clf__min_child_weight': np.arange(1, 10)\n",
    "}\n",
    "\n",
    "\n",
    "# Neural Network\n",
    "\n",
    "def get_new_model():\n",
    "    ###########################################################\n",
    "    # Your code goes here!\n",
    "    model = Sequential(name='Archibald')  # Models can be named!\n",
    "\n",
    "    # Add the first layer with 25 neurons\n",
    "    model.add(L.Dense(25, input_dim=X_train_prep.shape[1], kernel_initializer='random_normal'))\n",
    "\n",
    "    # Add activation function to the first layer\n",
    "    model.add(L.Activation('sigmoid'))\n",
    "\n",
    "    # Add another layer with 25 neurons\n",
    "    model.add(L.Dense(25, kernel_initializer='random_normal'))\n",
    "    model.add(L.Activation('sigmoid'))\n",
    "\n",
    "    # The output layer should produce probabilities for different classes\n",
    "    # Softmax activation function allows this transformation\n",
    "    # There will be 4 probabilities for the number of classes\n",
    "    model.add(L.Dense(2, activation='softmax', kernel_initializer='random_normal'))\n",
    "    ###########################################################\n",
    "\n",
    "    # Use Adam optimization algorithm\n",
    "    # It's a special gradient descent, we'll discuss it next time\n",
    "    optimizer = opt.Adam(lr=1e-3)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  metrics=[\"accuracy\"],\n",
    "                  optimizer=optimizer)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "clf_neuron = KerasClassifier(build_fn=get_new_model)\n",
    "\n",
    "\n",
    "param_dict_neural = {\n",
    "    'clf__batch_size': [10, 20, 40, 60, 80, 100],  # Define batch_size for the KerasClassifier\n",
    "    'clf__epochs': [10, 50, 100]  # Define epochs for the KerasClassifier\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "pipe_neuron = Pipeline([('scl', preprocessor),\n",
    "                        ('clf', clf_neuron)\n",
    "                        ])  # Pipeline with all steps\n",
    "\n",
    "\n",
    "# Defining pipelines for models\n",
    "pipe_logistic = Pipeline([('scl', preprocessor), ('clf', LogisticRegression(penalty=\"l2\", solver='liblinear'))])\n",
    "pipe_rndforest = Pipeline([('scl', preprocessor), ('clf', RandomForestClassifier(random_state=13))])\n",
    "pipe_knn = Pipeline([('scl', preprocessor), ('clf', KNeighborsClassifier())])\n",
    "pipe_catboost = Pipeline([('scl', preprocessor), ('clf', CatBoostClassifier(random_state=13))])\n",
    "pipe_xgb = Pipeline([('scl', preprocessor), ('clf', XGBClassifier(random_state=13))])\n",
    "pipe_neuron = Pipeline([\n",
    "    ('scl', preprocessor),\n",
    "    ('clf', clf_neuron)\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# RandomizedSearchCV for each model\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5)  # Define StratifiedKFold object with 5 splits\n",
    "\n",
    "logistic_randomized_pipe = RandomizedSearchCV(estimator=pipe_logistic, param_distributions=param_dict_logistic,\n",
    "                                              cv=skf, n_iter=30, n_jobs=-1)\n",
    "\n",
    "rndforest_randomized_pipe = RandomizedSearchCV(estimator=pipe_rndforest, param_distributions=param_dict_rndforest,\n",
    "                                               cv=skf, n_iter=30, n_jobs=-1)\n",
    "\n",
    "knn_randomized_pipe = RandomizedSearchCV(estimator=pipe_knn, param_distributions=param_dist_knn,\n",
    "                                         cv=skf, n_iter=30, n_jobs=-1)\n",
    "\n",
    "catboost_randomized_pipe = RandomizedSearchCV(estimator=pipe_catboost, param_distributions=param_dict_catboost,\n",
    "                                              cv=skf, n_iter=30, n_jobs=-1)\n",
    "\n",
    "xgb_randomized_pipe = RandomizedSearchCV(estimator=pipe_xgb, param_distributions=param_dict_xgb,\n",
    "                                         cv=skf, n_iter=30, n_jobs=-1)\n",
    "\n",
    "neuron_randomized_pipe = GridSearchCV(estimator=pipe_neuron, param_grid=param_dict_neural,\n",
    "                                      cv=skf, n_jobs=-1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-02-05T15:08:28.701696Z",
     "start_time": "2024-02-05T15:08:28.595695500Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "# Fitting the models\n",
    "neuron_randomized_pipe.fit(X_train, y_train)\n",
    "logistic_randomized_pipe.fit(X_train, y_train)\n",
    "rndforest_randomized_pipe.fit(X_train, y_train)\n",
    "knn_randomized_pipe.fit(X_train, y_train)\n",
    "catboost_randomized_pipe.fit(X_train, y_train)\n",
    "xgb_randomized_pipe.fit(X_train, y_train)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-02-05T16:53:03.446305800Z",
     "start_time": "2024-02-05T16:41:48.220200100Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Displaying the results\n",
    "models_quality = pd.DataFrame(columns=['Name', 'accuracy_score', 'recall_score', 'f1_score', 'precision_score'])\n",
    "\n",
    "models_names = [neuron_randomized_pipe, logistic_randomized_pipe, rndforest_randomized_pipe, knn_randomized_pipe,\n",
    "                catboost_randomized_pipe, xgb_randomized_pipe]\n",
    "\n",
    "models_string = ['neuron_randomized_pipe',\n",
    "                 'logistic_randomized_pipe', 'rndforest_randomized_pipe', 'knn_randomized_pipe',\n",
    "                 'catboost_randomized_pipe', 'xgb_randomized_pipe']\n",
    "\n",
    "models_quality_list = []\n",
    "\n",
    "for i in range(len(models_names)):\n",
    "    model = models_names[i]\n",
    "    log_pred = model.predict(X_holdout)\n",
    "\n",
    "    # Convert string representations of boolean values to actual boolean values\n",
    "    log_pred = log_pred.astype(bool)\n",
    "\n",
    "    model_quality = {\n",
    "        'Name': models_string[i],\n",
    "        'accuracy_score': accuracy_score(y_holdout, log_pred),\n",
    "        'recall_score': recall_score(y_holdout, log_pred),\n",
    "        'f1_score': f1_score(y_holdout, log_pred),\n",
    "        'precision_score': precision_score(y_holdout, log_pred)\n",
    "    }\n",
    "    models_quality_list.append(model_quality)\n",
    "\n",
    "models_quality = pd.concat([models_quality, pd.DataFrame(models_quality_list)], ignore_index=True)\n",
    "\n",
    "\n",
    "# Printing accuracy scores for each model\n",
    "for model_name, model in zip(models_string, models_names):\n",
    "    print(f\"{model_name}: {accuracy_score(y_holdout, model.predict(X_holdout).astype(bool))}\")\n",
    "\n",
    "# Storing prediction values for each model\n",
    "prediction_values = pd.DataFrame()\n",
    "for i, model in enumerate(models_names, 1):\n",
    "    prediction_values[str(i)] = model.predict(X_holdout)\n",
    "\n",
    "# Converting boolean predictions to binary (0 or 1)\n",
    "prediction_values = prediction_values.replace([True], 1)\n",
    "prediction_values = prediction_values.replace([False], 0)\n",
    "\n",
    "# Computing the sum of predictions across models and rounding to get final predictions\n",
    "\n",
    "\n",
    "prediction_values = prediction_values.replace({'True': True, 'False': False})\n",
    "prediction_values = prediction_values.astype(int)\n",
    "prediction_values['sum'] = np.round(prediction_values.sum(axis=1) / len(models_names)).astype(int)\n",
    "\n",
    "prediction_values\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-05T17:16:10.363543Z",
     "start_time": "2024-02-05T17:16:07.445544700Z"
    }
   },
   "execution_count": 40,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "# Printing accuracy score for combined predictions\n",
    "print('Accuracy score for combined predictions:', accuracy_score(y_holdout, prediction_values['sum']))\n",
    "\n",
    "# Calculating accuracy score for the Random Forest model\n",
    "rndforest_accuracy = accuracy_score(y_holdout, rndforest_randomized_pipe.predict(X_holdout))\n",
    "print('Accuracy score for Random Forest model:', rndforest_accuracy)\n",
    "\n",
    "# Storing predictions for the test data using the Random Forest model\n",
    "prediction_test_res = pd.DataFrame()\n",
    "prediction_test_res['Transported'] = rndforest_randomized_pipe.predict(data_pre_test)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-02-05T17:16:20.163403200Z",
     "start_time": "2024-02-05T17:16:20.043386500Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "# Create a DataFrame for test predictions with PassengerId\n",
    "prediction_test = pd.DataFrame()\n",
    "prediction_test['PassengerId'] = data_raw_test['PassengerId']\n",
    "prediction_test['Transported'] = prediction_test_res['Transported']\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "prediction_test.to_csv('result.csv', index=False)\n",
    "\n",
    "# Display the DataFrame\n",
    "prediction_test\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-02-05T17:17:02.006062100Z",
     "start_time": "2024-02-05T17:17:01.980061500Z"
    }
   },
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
