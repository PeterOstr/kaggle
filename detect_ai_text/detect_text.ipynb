{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-15T10:21:29.956961500Z",
     "start_time": "2023-12-15T10:21:29.931962200Z"
    }
   },
   "id": "6ba63909f01b6deb",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "data_essays = pd.read_csv('data/train_essays.csv')\n",
    "data_essays_test = pd.read_csv('data/test_essays.csv')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-15T13:18:59.301875400Z",
     "start_time": "2023-12-15T13:18:59.229876500Z"
    }
   },
   "id": "f658d4440e51c4d1",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "data_essays"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-15T13:17:00.104066700Z",
     "start_time": "2023-12-15T13:17:00.065064800Z"
    }
   },
   "id": "e9ef9210517e187",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-15T13:15:09.029421800Z",
     "start_time": "2023-12-15T13:15:07.808422200Z"
    }
   },
   "id": "44495ac04c5e68cf",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "# Предполагается, что у вас есть данные, содержащие тексты и соответствующих им авторов.\n",
    "# Предполагается, что вы уже предобработали тексты и собрали их в формате списка.\n",
    "\n",
    "# Пример данных\n",
    "texts = data_essays['text'].tolist()\n",
    "authors = data_essays['generated'].tolist()\n",
    "\n",
    "# # Используйте LabelEncoder для преобразования меток авторов в числа\n",
    "# label_encoder = LabelEncoder()\n",
    "# encoded_authors = label_encoder.fit_transform(authors)\n",
    "\n",
    "# Обучите Word2Vec модель на ваших текстах (здесь приведен пример)\n",
    "# Мы предполагаем, что тексты были предварительно обработаны (токенизированы, удалены стоп-слова и т.д.)\n",
    "word2vec_model = Word2Vec(sentences=texts, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Получите векторы для каждого текста, усредняя векторы его слов\n",
    "def get_text_vector(text):\n",
    "    vectors = [word2vec_model.wv[word] for word in text.split() if word in word2vec_model.wv]\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(word2vec_model.vector_size)\n",
    "\n",
    "# Преобразуйте все тексты в векторы\n",
    "text_vectors = [get_text_vector(text) for text in texts]\n",
    "\n",
    "# Разделите данные на обучающий и тестовый наборы\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_vectors, authors, test_size=0.2, random_state=42)\n",
    "\n",
    "# Создайте модель нейронной сети\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=word2vec_model.vector_size, activation='relu'))\n",
    "model.add(Dense(64, input_dim=word2vec_model.vector_size, activation='relu'))\n",
    "model.add(Dense(len(np.unique(authors)), activation='softmax'))\n",
    "\n",
    "# Скомпилируйте модель\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Обучите модель\n",
    "model.fit(np.array(X_train), np.array(y_train), epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Оцените модель\n",
    "loss, accuracy = model.evaluate(np.array(X_test), np.array(y_test))\n",
    "print(f'Loss: {loss}, Accuracy: {accuracy}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-16T10:02:34.485137100Z",
     "start_time": "2023-12-16T10:02:25.472651Z"
    }
   },
   "id": "540098c39db80a0d",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "model.predict(np.array(X_test))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-16T10:38:45.624247200Z",
     "start_time": "2023-12-16T10:38:45.344247900Z"
    }
   },
   "id": "8a34fdc1e9d5f4e5",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "X_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-16T10:39:52.911758700Z",
     "start_time": "2023-12-16T10:39:52.567705100Z"
    }
   },
   "id": "49e3b2bf94c472d7",
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "/"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cdca111e83086f29",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
